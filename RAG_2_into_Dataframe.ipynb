{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**# Install Librarys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain chromadb pypdf ollama tiktoken sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\LENOVO\\Downloads\\Danone_Annual Results\\2021_Danone_test.pdf\n",
      "Total Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pypdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define folder where PDFs are stored\n",
    "pdf_folder = r\"C:\\Users\\LENOVO\\Downloads\\Danone_Annual Results\"\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    all_text = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder_path)):  # Ensure order (2021, 2022, 2023)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            \n",
    "            pdf_reader = pypdf.PdfReader(pdf_path)\n",
    "            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])\n",
    "            all_text.append({\"filename\": filename, \"text\": text})\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "# Extract text from PDFs\n",
    "documents = extract_text_from_pdfs(pdf_folder)\n",
    "\n",
    "# Split text into chunks for embedding\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"text\"])\n",
    "    for chunk in splits:\n",
    "        chunks.append({\"text\": chunk, \"source\": doc[\"filename\"]})\n",
    "\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Embed & Store Data in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Documents stored in ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import shutil\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "#If the database is corrupted or locked, it can crash the kernel.\n",
    "# Fix: Delete and reinitialize the database:\n",
    "shutil.rmtree(\"./chroma_db\", ignore_errors=True)  # Delete old DB\n",
    "\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Persistent storage\n",
    "collection = chroma_client.get_or_create_collection(name=\"danone_annual_results_3\")\n",
    "\n",
    "# Load Sentence Transformer model for embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store chunks in ChromaDB\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        ids=[str(idx)],\n",
    "        documents=[chunk[\"text\"]],\n",
    "        metadatas=[{\"source\": chunk[\"source\"]}]\n",
    "    )\n",
    "\n",
    "print(\"✅ Documents stored in ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrival From Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context:\n",
      " 35 36\n",
      "2020KPIs 2021\n",
      "Net Sales(1)\n",
      "LFL Sales Growth(1)\n",
      "Free Cash-Flow(1)\n",
      "Recurring Operating Margin(1)\n",
      "Net debt/ EBITDA\n",
      "ROIC\n",
      "Dividend per share\n",
      "E-commerce\n",
      "€23.62 BN\n",
      "-1.5%\n",
      "€2.1 BN\n",
      "14.00%\n",
      "2.8x\n",
      "8.5%\n",
      "€1.94\n",
      "+40 vs L Y (representing \n",
      "10% of total revenue)\n",
      "+16% vs L Y (representing \n",
      "10% of total revenue)\n",
      "€24.3 BN\n",
      "+3.4%\n",
      "€2.5 BN\n",
      "13.74%\n",
      "3x\n",
      "8.7%\n",
      "€1.94\n",
      "OUR PERFORMANCE IN 2021\n"
     ]
    }
   ],
   "source": [
    "def query_chroma(query_text, top_k=3):\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_texts = [doc for doc in results['documents'][0]]\n",
    "    return \"\\n\".join(retrieved_texts)\n",
    "\n",
    "query_text = \"What is Danone's sales revenue for the last 2 years?\"\n",
    "context = query_chroma(query_text)\n",
    "print(\"Retrieved Context:\\n\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use llama 3.2 to retrieve sales figure and put into PD dataframe for further integration in DB/PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Sales Data:\n",
      "    Year  Net Sales\n",
      "0  2020      23.62\n",
      "1  2021      24.30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "\n",
    "# ✅ Function to extract last 2 years' sales data automatically\n",
    "def extract_sales_llm():\n",
    "    # Query all stored documents\n",
    "    results = collection.query(query_texts=[\"Company X annual report\"], n_results=2)\n",
    "\n",
    "    # Combine all retrieved text\n",
    "    full_text = \" \".join([doc[0] for doc in results[\"documents\"]])\n",
    "\n",
    "    # LLaMA 3.2 prompt to extract last 2 years' sales\n",
    "    prompt = f\"\"\"\n",
    "    From the following financial report text, identify the last two years of Net Sales data.\n",
    "    Extract the years and corresponding sales figures in billions.\n",
    "\n",
    "    Financial Report Text:\n",
    "    {full_text}\n",
    "\n",
    "    Return the data in this format:\n",
    "    Year: 2020, Net Sales: 27.5\n",
    "    Year: 2021, Net Sales: 25.8\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Query LLaMA 3.2\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    extracted_text = response[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Parse extracted sales data\n",
    "    sales_data = []\n",
    "    for line in extracted_text.split(\"\\n\"):\n",
    "        if \"Year:\" in line and \"Net Sales:\" in line:\n",
    "            parts = line.split(\", \")\n",
    "            year = parts[0].split(\": \")[1].strip()\n",
    "            sales = float(parts[1].split(\": \")[1].strip())\n",
    "            sales_data.append({\"Year\": year, \"Net Sales\": sales})\n",
    "\n",
    "    return sales_data\n",
    "\n",
    "# ✅ Extract sales data dynamically\n",
    "sales_results = extract_sales_llm()\n",
    "\n",
    "# ✅ Convert to DataFrame\n",
    "df_sales = pd.DataFrame(sales_results)\n",
    "\n",
    "# ✅ Print DataFrame\n",
    "print(\"Extracted Sales Data:\\n\", df_sales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
